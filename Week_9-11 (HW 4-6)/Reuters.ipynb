{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas_datareader.data as web\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 "
     ]
    }
   ],
   "source": [
    "# grab links to news articles from reuter's archive page\n",
    "# ten+ articles are displayed on each page\n",
    "url_links = []\n",
    "for i in range(1500,3277):\n",
    "    try:\n",
    "        url = 'https://www.reuters.com/news/archive/marketsNews?view=page&page=' + str(i) + '&pageSize=10'\n",
    "#         print(url)\n",
    "        html = requests.get(url)\n",
    "        content = html.content\n",
    "        content.decode().strip().replace('\\t','').split('\\n')\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        for tags in soup.find_all('a'):\n",
    "            if re.search('article', tags['href']):\n",
    "                url_links.append(tags['href'])\n",
    "        print(i, end = ' ')\n",
    "    except:\n",
    "        continue\n",
    "            \n",
    "# some linkes may be duplicated thus we only select those that only appear once\n",
    "final_urls = []\n",
    "for url in url_links:\n",
    "    if url not in final_urls:\n",
    "        final_urls.append(url)\n",
    "print('Articles Count:', len(final_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# retreive the title, publish time and content for each article\n",
    "\n",
    "title_all = []\n",
    "time_all = []\n",
    "content_all = []\n",
    "url_all = []\n",
    "\n",
    "count = 0\n",
    "for url in final_urls:\n",
    "    try:\n",
    "        link = 'https://www.reuters.com' + url\n",
    "        page = requests.get(link).content\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        newsTitle = soup.title.text\n",
    "        print(count, end = ' ')\n",
    "        newsTime = soup.find_all(\"div\", {\"class\": 'ArticleHeader_date'})[0].text\n",
    "        newsContent = ''\n",
    "        for tag in soup.find_all('p'):\n",
    "            newsContent += tag.text\n",
    "\n",
    "        title_all.append(newsTitle)\n",
    "        time_all.append(newsTime)\n",
    "        content_all.append(newsContent)\n",
    "        url_all.append(link)\n",
    "        file = pd.DataFrame({'Title' : title_all, 'Time':time_all, 'Content':content_all, 'Link':url_all})\n",
    "        file['Date'] = [x.split('/')[0] for x in file['Time'].tolist()]\n",
    "        file['Date'] = pd.to_datetime(file['Date'])\n",
    "        file.sort_values(['Date'], inplace = True)\n",
    "\n",
    "        file['Len'] = [len(x) for x in file['Content']]\n",
    "        file = file[file['Len'] >= 600]\n",
    "\n",
    "        file.to_csv('Reuters_Articles.csv')\n",
    "        count += 1\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# remove spaces infront of titles\n",
    "title_all = [x.lstrip() for x in title_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save all articles to one csv file\n",
    "# file = pd.DataFrame({'Title' : title_all, 'Time':time_all, 'Content':content_all, 'Link':url_all})\n",
    "# file['Date'] = [x.split('/')[0] for x in file['Time'].tolist()]\n",
    "# file['Date'] = pd.to_datetime(file['Date'])\n",
    "# file.sort_values(['Date'], inplace = True)\n",
    "\n",
    "# file['Len'] = [len(x) for x in file['Content']]\n",
    "# file = file[file['Len'] >= 600]\n",
    "\n",
    "# file.to_csv('Reuters_Articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
